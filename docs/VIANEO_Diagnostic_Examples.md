# Vianeo Main Diagnostic - Examples

## Executive Diagnostic Section Examples

### Strengths Examples

#### ✅ GOOD Example (Rayla AI)

"Rayla AI demonstrates strong technical differentiation with patent-pending multi-model orchestration and grant-specific training data, creating barriers difficult for generic AI competitors to replicate. Founder brings rare 10+ year domain expertise in grant strategy, validated through prior service clients who secured millions in funding. Early traction is compelling: 50+ beta users show 60% time reduction and 30% quality improvement, 2 consulting partnerships provide channel validation, 500+ organization waitlist signals market demand, and 1 documented funded grant proves real-world efficacy."

**Why it's good:**
- ✓ Specific technical advantages (patent-pending orchestration, training data)
- ✓ Concrete numbers (10+ years, 60% reduction, 50+ users, 2 partnerships, 500+ waitlist)
- ✓ Names and specifics (Rayla AI, grant strategy, consulting partnerships)
- ✓ Evidence hierarchy (external validation: funded grant, partnerships, beta users)
- ✓ Leads with strongest point (technical differentiation)
- ✓ Dense with facts, no fluff

#### ❌ BAD Example

"The team has good experience and the technology seems promising. Early customer feedback has been positive and the market opportunity appears large. The founders are committed to the vision."

**Why it's bad:**
- ✗ Generic language ("good", "seems", "appears")
- ✗ No specific numbers or evidence
- ✗ Vague claims without support
- ✗ Could apply to any business
- ✗ No concrete validation
- ✗ Hedging language ("seems", "appears")

---

### Risks Examples

#### ✅ GOOD Example (Rayla AI)

"Revenue model remains entirely unvalidated with zero paying customers despite operational beta, indicating critical pricing and willingness-to-pay uncertainty. Team structure creates execution risk: CTO lacks distinctive AI/ML credentials, development outsourced to contractors, no technical co-founder, and missing nonprofit executive or regulatory expertise. Founder focus fragmented across Intention Labs agency, Impact Partners Network, and Rayla AI, raising sustainability concerns."

**Why it's good:**
- ✓ Direct, specific language ("zero paying customers", "fewer than 5 interviews")
- ✓ Quantified risks (zero revenue, fragmented focus across 3 ventures)
- ✓ Systemic issues identified (team structure, founder focus, validation gaps)
- ✓ Connected to business impact (pricing uncertainty, execution risk, sustainability concerns)
- ✓ Prioritized by severity (revenue first, then team, then focus)
- ✓ No hedging ("may", "might", "possibly")

#### ❌ BAD Example

"There are some concerns about market adoption and the team could benefit from additional startup experience. Manufacturing timelines may need optimization. The competitive landscape is evolving and requires monitoring."

**Why it's bad:**
- ✗ Hedging language ("some concerns", "may need", "could benefit")
- ✗ Vague and generic
- ✗ No quantification
- ✗ Downplays problems
- ✗ No urgency or severity
- ✗ Could apply to any business

---

### Near-term Actions Examples

#### ✅ GOOD Example (Rayla AI)

"Shawna should immediately convert 20+ beta users to paid subscriptions (target 40% conversion rate) to validate $20-50 ARPU pricing hypothesis and establish first revenue proof point. Leadership team (Shawna, Soufiane) must conduct 40-60 structured customer interviews (5-10 per segment: Development Directors, Grant Coordinators, Startup Founders, Independent Consultants, University Administrators, Municipal Officers) using standardized protocol to validate needs qualification matrix and refine ICP. Shawna should clarify founder time allocation and establish plan to wind down or delegate Intention Labs and IPN within 90 days, demonstrating exclusive focus required for venture-scale execution."

**Why it's good:**
- ✓ Clear owners (Shawna, Leadership team)
- ✓ Specific, measurable actions (20+ users, 40% conversion, 40-60 interviews)
- ✓ Time-bound (within 90 days)
- ✓ Delegation-ready language
- ✓ Clear outcomes (validate pricing, refine ICP, demonstrate focus)
- ✓ Addresses critical risks identified in Risks section

#### ❌ BAD Example

"The team should talk to more customers to better understand the market. Consider adjusting pricing strategy. Continue building the product and exploring partnerships. Think about hiring additional team members."

**Why it's bad:**
- ✗ No owners assigned
- ✗ Vague actions ("talk to more", "consider", "think about")
- ✗ No measurable outcomes
- ✗ No timeframes
- ✗ Not delegation-ready
- ✗ Hedging language ("consider", "think about")

---

### Evidence Gaps Examples

#### ✅ GOOD Example (Rayla AI)

"Customer interview methodology completely undocumented (sample sizes unclear, no validated needs prioritization framework, buyer vs. user distinction unmapped). Competitive intelligence lacks depth: no firsthand testing of Instrumentl or Grantable, pricing comparison incomplete, feature parity unassessed, churn drivers unknown. Financial validation absent: unit economics based on projections not actual customer data, CAC unmeasured across channels, LTV assumptions unproven, burn rate and runway undisclosed despite fundraising dependency."

**Why it's good:**
- ✓ Specific about what's missing (sample sizes, needs framework, feature parity, CAC, vesting)
- ✓ Grouped logically by category (customer, competitive, financial, team, ecosystem)
- ✓ Material gaps that affect decision-making
- ✓ Clear why gaps matter (e.g., "despite fundraising dependency")
- ✓ Names specific tools and entities (Instrumentl, Grantable)
- ✓ Different from information in Risks section

#### ❌ BAD Example

"Some additional documentation would be helpful. More customer interviews could provide useful insights. Team bios could be more detailed. Additional market research would strengthen the analysis."

**Why it's bad:**
- ✗ Vague ("some additional", "could provide", "would strengthen")
- ✗ Not specific about what's missing
- ✗ Doesn't explain why gaps matter
- ✗ Generic suggestions
- ✗ Minor documentation issues, not material gaps
- ✗ Repetitive and unhelpful

---

## Dimension Summary Examples

### ✅ GOOD Table

| Dimension | Score | Interpretation |
|-----------|-------|----------------|
| **Legitimacy** | 3.5/5 | Above Average - Real problem, genuine advantages, but team/financial gaps |
| **Desirability** | 3.5/5 | Promising - Strong early signals, but thin evidence base requires validation |
| **Acceptability** | 3.0/5 | Developing - Mixed ecosystem, key influencer positions undefined |
| **Feasibility** | 3.0/5 | Developing - Technical capability adequate, team execution risk high |
| **Viability** | 2.5/5 | Problematic - Pre-revenue, unvalidated unit economics, fundraising dependent |

**Overall Status:** Promising opportunity requiring significant de-risking before institutional investment. Strong market fit and differentiated technology offset by execution gaps and unvalidated business model.

**Why it's good:**
- ✓ Status keywords match score ranges
- ✓ Brief explanations specific to the venture
- ✓ Overall status balances strengths and weaknesses
- ✓ Clear implications for next steps

### ❌ BAD Table

| Dimension | Score | Interpretation |
|-----------|-------|----------------|
| **Legitimacy** | 3.5/5 | Good |
| **Desirability** | 3.5/5 | Okay |
| **Acceptability** | 3.0/5 | Average |
| **Feasibility** | 3.0/5 | Moderate |
| **Viability** | 2.5/5 | Needs work |

**Overall Status:** The project shows promise but has areas for improvement.

**Why it's bad:**
- ✗ Generic status words don't match framework
- ✗ No specific explanations
- ✗ Overall status too vague
- ✗ No actionable implications

---

## Critical Path Examples

### ✅ GOOD Critical Path Structure

**Immediate Priority (Weeks 1-4)**
1. Launch paid tiers to beta cohort, target 20+ conversions
2. Begin systematic customer interview program (10 interviews/week)
3. Document team equity structure and founder time allocation plan

**Short-term Priority (Months 2-3)**
1. Achieve $10K MRR from paying customers (200+ users at $50 ARPU or 100+ at $100)
2. Complete 40-60 customer interviews across all requester segments
3. Internalize technical development or hire VP Engineering
4. Test competitive solutions firsthand and update positioning

**Medium-term Priority (Months 4-6)**
1. Demonstrate clear founder focus (Intention Labs/IPN transitioned)
2. Validate unit economics with actual cohort data (CAC, LTV, churn)
3. Strengthen IP position (file utility patent, document FTO analysis)
4. Map ecosystem gatekeepers and develop regulatory strategy

**Success Metrics**
- 40%+ beta-to-paid conversion rate
- <$300 CAC validated across channels
- LTV:CAC ratio >3:1 within 18 months
- <10% monthly churn rate
- NPS >50 from paying customers
- 10+ documented funded grants by end of Year 1

**Why it's good:**
- ✓ Proper time horizons (weeks, then months)
- ✓ Specific, measurable actions
- ✓ Realistic item counts (3-4 per section)
- ✓ Metrics have specific thresholds
- ✓ Clear progression and dependencies

### ❌ BAD Critical Path Structure

**Next Steps**
- Talk to customers
- Improve the product
- Raise money
- Hire team members
- Build partnerships
- Increase revenue

**Goals**
- Get more users
- Better product-market fit
- Strong team
- Good metrics

**Why it's bad:**
- ✗ No time horizons
- ✗ Vague actions
- ✗ No measurable outcomes
- ✗ No prioritization
- ✗ Metrics not specific
- ✗ Too many items without focus

---

## Key Takeaways

**Make it Specific:**
- Use actual names (Shawna, Soufiane, not "the team")
- Include real numbers (50+ beta users, not "some traction")
- Reference actual things (Instrumentl, Grantable, not "competitors")

**Make it Direct:**
- Say "zero paying customers" not "limited revenue traction"
- Say "completely undocumented" not "could use more documentation"
- Say "must conduct 40-60 interviews" not "should consider more customer research"

**Make it Actionable:**
- Include owners (Shawna should...)
- Include outcomes (to validate pricing hypothesis)
- Include timeframes (within 90 days)

**Make it Evidence-Based:**
- Every claim backed by assessment data
- Distinguish validated from assumed
- Flag gaps honestly

---

**Remember:** Good examples are specific, direct, actionable, and evidence-based. Bad examples are vague, generic, passive, and aspirational.
